{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project name:** *AEGIS*\n",
    "\n",
    "**Team Name:** *Black Cat* \n",
    "\n",
    "**Team Members:** *Sumit Singh - Vibecoder and AI/ML Engineer*\n",
    "\n",
    "\n",
    "**Problem statement**\n",
    "*In many rural or under-resourced healthcare settings, doctors lack immediate support to comprehensively interpret patient history, potential complications, or diagnostic clues. This can lead to missed or delayed diagnoses, worsening patient outcomes. I aim to bridge this gap with an AI-assisted voice companion that helps doctors analyze patient cases in real-time, ensuring they don‚Äôt miss key details‚Äîeven in resource-limited environments.*\n",
    "\n",
    "**Overall solution**\n",
    "*To solve this, I built AEGIS‚Äîa multi-agent AI operating system that fits right into a doctor's pocket. By utilizing Google's **Med-Gemma 1.5 (4B)** model, I created \"Suvi\", a conversational AI Clinical Nurse.* \n",
    "\n",
    "*The effective use of this medical-grade model lies in its Agent-to-Agent (A2A) architecture. Instead of typing into clunky EHR systems, doctors can use Voice-to-Text to consult with Suvi hands-free while actively examining a patient. Suvi uses Retrieval-Augmented Generation (RAG) to instantly pull the patient's past medical history from our database and cross-reference it with current symptoms.* \n",
    "\n",
    "Beyond just chat, the solution includes:\n",
    "* **üëÅÔ∏è Aegis-Vision:** A computer vision agent that uses a quick face scan to instantly securely retrieve a patient's medical file.\n",
    "* **üó£Ô∏è Suvi Voice:** The core clinical reasoning agent that acts as a real-time sounding board for the doctor.\n",
    "* **‚úçÔ∏è Aegis-Scribe:** A background agent that takes the unstructured voice transcript and automatically generates a structured, ready-to-sign Word Document report, eliminating administrative overhead.\n",
    "\n",
    "**Technical details**\n",
    "*Product feasibility and deployment in low-resource environments were my top priorities. Building a massive AI app is useless if a rural clinic can't afford the hardware to run it.* \n",
    "\n",
    "*That is why I specifically engineered the backend to utilize the **Med-Gemma 4B** model rather than the heavier 27B version. The 4B model runs flawlessly on a single, low-cost T4 GPU (which I hosted via Kaggle for this build), making cloud-hosting financially viable for under-resourced clinics.* \n",
    "\n",
    "**The Tech Stack:**\n",
    "* **Frontend:** Built in Flutter, ensuring it runs smoothly on standard, low-cost Android tablets or smartphones already present in clinics.\n",
    "* **Backend:** Python and FastAPI, exposed via Ngrok for seamless mobile-to-cloud communication.\n",
    "* **Database:** Supabase (PostgreSQL) handles our vector database for storing patient records, ensuring lightning-fast RAG retrieval.\n",
    "* **AI Protocol:** Because Med-Gemma is a reasoning model, it naturally outputs its internal \"scratchpad\" thoughts. To maintain a professional UI, I implemented a strict Regex-based preprocessing layer that traps and scrubs the model's `<think>...</think>` tags on the backend. The doctor only ever sees and hears the clean, final clinical advice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T07:52:22.339583Z",
     "iopub.status.busy": "2026-02-23T07:52:22.339288Z",
     "iopub.status.idle": "2026-02-23T07:52:41.870688Z",
     "shell.execute_reply": "2026-02-23T07:52:41.869990Z",
     "shell.execute_reply.started": "2026-02-23T07:52:22.339549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " INITIALIZING AEGIS BACKEND ENVIRONMENT...\n",
      " Dependencies Installed.\n",
      "\n",
      " HARDWARE ACCELERATION STATUS:\n",
      "   ‚Ä¢ CUDA Available: YES\n",
      "   ‚Ä¢ GPU Count: 2\n",
      "   ‚Ä¢ GPU 0: Tesla T4 (15.64 GB VRAM)\n",
      "   ‚Ä¢ GPU 1: Tesla T4 (15.64 GB VRAM)\n"
     ]
    }
   ],
   "source": [
    "#  CELL 1: ENVIRONMENT SETUP & HARDWARE OPTIMIZATION\n",
    "# ==============================================================================\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "print(\" INITIALIZING AEGIS BACKEND ENVIRONMENT...\")\n",
    "\n",
    "!pip install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q -U transformers accelerate bitsandbytes peft\n",
    "!pip install -q -U fastapi uvicorn python-multipart nest_asyncio pyngrok\n",
    "!pip install -q -U sentence-transformers supabase\n",
    "!pip install -q face_recognition opencv-python-headless numpy\n",
    "\n",
    "print(\" Dependencies Installed.\")\n",
    "\n",
    "import torch\n",
    "print(f\"\\n HARDWARE ACCELERATION STATUS:\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"   ‚Ä¢ CUDA Available: YES\")\n",
    "    print(f\"   ‚Ä¢ GPU Count: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"   ‚Ä¢ GPU {i}: {torch.cuda.get_device_name(i)} ({round(torch.cuda.get_device_properties(i).total_memory/1e9, 2)} GB VRAM)\")\n",
    "else:\n",
    "    print(\" CRITICAL ERROR: No GPU detected. Change Accelerator to 'GPU T4 x2'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Global Configuration & Database Initialization\n",
    "This cell establishes connections to Supabase (Vector DB) and Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T07:52:41.872082Z",
     "iopub.status.busy": "2026-02-23T07:52:41.871728Z",
     "iopub.status.idle": "2026-02-23T07:52:50.532736Z",
     "shell.execute_reply": "2026-02-23T07:52:50.532025Z",
     "shell.execute_reply.started": "2026-02-23T07:52:41.872053Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Authenticated with Hugging Face.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c9b3c6799a47d9a4157e6f6ef37bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mMPNetModel LOAD REPORT\u001b[0m from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Database & Embedding Engine Online.\n"
     ]
    }
   ],
   "source": [
    "#  CELL 2: CREDENTIALS, SUPABASE & EMBEDDINGS\n",
    "# ==============================================================================\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from huggingface_hub import login\n",
    "from supabase import create_client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    NGROK_TOKEN = user_secrets.get_secret(\"NGROK_TOKEN\")\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\" Authenticated with Hugging Face.\")\n",
    "except Exception as e:\n",
    "    print(f\" CREDENTIAL WARNING: Could not load secrets.\\nError: {e}\")\n",
    "\n",
    "SUPABASE_URL = \"https://ceesnsewtbkouxjnzwqc.supabase.co\" \n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImNlZXNuc2V3dGJrb3V4am56d3FjIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NzEzNzMyMzgsImV4cCI6MjA4Njk0OTIzOH0.19fxCmQ4iStEJo0_tq5j2PDtxInIVKLLFfTZMcfMq94\"\n",
    "\n",
    "try:\n",
    "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "    embedder = SentenceTransformer('all-mpnet-base-v2') \n",
    "    print(\" Database & Embedding Engine Online.\")\n",
    "except Exception as e:\n",
    "    print(f\" DATABASE CONNECTION ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The LLM Engine (Med-Gemma)\n",
    "Loading the LLM into VRAM using 4-bit quantization and auto-sharding across GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T07:52:50.534707Z",
     "iopub.status.busy": "2026-02-23T07:52:50.534046Z",
     "iopub.status.idle": "2026-02-23T07:53:02.982117Z",
     "shell.execute_reply": "2026-02-23T07:53:02.981201Z",
     "shell.execute_reply.started": "2026-02-23T07:52:50.534678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading Tokenizer & Weights for google/medgemma-1.5-4b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efd555ba55d4fe28359e8f26e1c6ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SYSTEM ONLINE. VRAM Footprint: 3.17 GB\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: MODEL LOADER \n",
    "# ==============================================================================\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURATION: CHOOSE YOUR Model\n",
    "# ------------------------------------------------------------------------------\n",
    "# FOR JUDGES: To test maximum reasoning capabilities, uncomment the 27B model below. \n",
    "# Requires A100 (40GB) VRAM.\n",
    "# TARGET_MODEL = \"google/medgemma-27b-it\" \n",
    "\n",
    "TARGET_MODEL = \"google/medgemma-1.5-4b-it\" \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "def load_medical_brain(model_id):\n",
    "    try:\n",
    "        print(f\"\\n Loading Tokenizer & Weights for {model_id}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\", \n",
    "            token=HF_TOKEN,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        return model, tokenizer, True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERROR LOADING {model_id}: {e}\")\n",
    "        return None, None, False\n",
    "\n",
    "model, tokenizer, success = load_medical_brain(TARGET_MODEL)\n",
    "\n",
    "if success:\n",
    "    print(f\"\\n SYSTEM ONLINE. VRAM Footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Specialized Agents & Tools\n",
    "This defines the distinct tools available to our Orchestrator:\n",
    "1. RAG Search\n",
    "2. Vitals Updating\n",
    "3. Face Identification (Aegis-Vision)\n",
    "4. Report Generation (Aegis-Scribe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T07:53:02.984270Z",
     "iopub.status.busy": "2026-02-23T07:53:02.983524Z",
     "iopub.status.idle": "2026-02-23T07:53:04.591847Z",
     "shell.execute_reply": "2026-02-23T07:53:04.591232Z",
     "shell.execute_reply.started": "2026-02-23T07:53:02.984215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#  CELL 4: SPECIALIZED AGENTS & TOOLS\n",
    "# ==============================================================================\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# --- TOOL 1: RAG MEMORY RECALL ---\n",
    "def search_patient_history(patient_id, query):\n",
    "    query_vector = embedder.encode(query).tolist()\n",
    "    response = supabase.rpc(\n",
    "        'match_clinical_records',\n",
    "        {\n",
    "            'query_embedding': query_vector, \n",
    "            'match_threshold': 0.1, \n",
    "            'match_count': 3,\n",
    "            'filter_patient_id': int(patient_id)\n",
    "        }\n",
    "    ).execute()\n",
    "    \n",
    "    if not response.data:\n",
    "        return \"No relevant history found.\"\n",
    "        \n",
    "    context_str = \"\"\n",
    "    for idx, record in enumerate(response.data):\n",
    "        text = record.get('content', record.get('content_markdown', ''))\n",
    "        context_str += f\"--- RECORD {idx+1} ---\\n{text}\\n\"\n",
    "    return context_str\n",
    "\n",
    "# --- TOOL 2: VITALS UPDATER ---\n",
    "def update_patient_vitals(patient_id, field, value):\n",
    "    try:\n",
    "        valid_fields = ['weight_kg', 'height_cm', 'blood_group', 'allergies']\n",
    "        if field not in valid_fields:\n",
    "            return f\"Error: Cannot update '{field}'.\"\n",
    "        supabase.table('patients').update({field: value}).eq('id', patient_id).execute()\n",
    "        return f\"Success. {field} updated.\"\n",
    "    except Exception as e:\n",
    "        return f\"Database Error: {e}\"\n",
    "\n",
    "# --- TOOL 3: AEGIS-VISION (FACE ID) ---\n",
    "def identify_patient_from_image(image_bytes):\n",
    "    try:\n",
    "        # Convert bytes to OpenCV format\n",
    "        nparr = np.frombuffer(image_bytes, np.uint8)\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        encodings = face_recognition.face_encodings(rgb_img)\n",
    "        if len(encodings) == 0:\n",
    "            return {\"status\": \"failed\", \"message\": \"No face detected in image.\"}\n",
    "            \n",
    "        query_embedding = encodings[0].tolist()\n",
    "        \n",
    "        # Match in Supabase\n",
    "        response = supabase.rpc(\n",
    "            'match_patient_face', \n",
    "            {'query_embedding': query_embedding, 'match_threshold': 0.90, 'match_count': 1}\n",
    "        ).execute()\n",
    "        \n",
    "        data = response.data\n",
    "        if len(data) > 0:\n",
    "            return {\"status\": \"success\", \"patient_id\": data[0]['id'], \"name\": data[0]['full_name']}\n",
    "        else:\n",
    "            return {\"status\": \"failed\", \"message\": \"Patient face not found in database.\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# --- TOOL 4: AEGIS-SCRIBE (REPORT GENERATOR) ---\n",
    "def generate_clinical_report(transcript):\n",
    "    prompt = f\"\"\"You are Aegis-Scribe, a strict medical reporting AI. \n",
    "Extract the clinical findings from the following transcript and format them professionally. \n",
    "Do not include conversational filler, greetings, or your internal thoughts.\n",
    "\n",
    "[TRANSCRIPT BEGIN]\n",
    "{transcript}\n",
    "[TRANSCRIPT END]\n",
    "\n",
    "FORMAT REQUIRED:\n",
    "Provide a clear, structured summary including: Chief Complaint, Findings, and Recommendations.\"\"\"\n",
    "    \n",
    "    full_prompt = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=400, temperature=0.1)\n",
    "        \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"model\\n\")[-1].strip()\n",
    "\n",
    "# --- UTILITY: TEXT CLEANER ---\n",
    "def clean_llm_output(raw_text):\n",
    "    cleaned = raw_text\n",
    "    \n",
    "    # NEW: Strip out EVERYTHING inside <think>...</think> tags (spanning multiple lines)\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', cleaned, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # 1. Strip out the specific <unused94>thought blocks and EVERYTHING until a double newline\n",
    "    cleaned = re.sub(r'<[^>]*>thought[\\s\\S]*?(?=\\n\\n|\\Z)', '', cleaned, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 2. Strip out numbered reasoning lists (e.g., \"1. Identify the persona...\")\n",
    "    cleaned = re.sub(r'(?i)(?:1\\.\\s+Identify|Identify the persona|Identify the input)[\\s\\S]*?(?=\\n\\n|\\Z)', '', cleaned)\n",
    "    \n",
    "    # 3. Remove any remaining XML tags\n",
    "    cleaned = re.sub(r'<[^>]*>', '', cleaned)\n",
    "    \n",
    "    # 4. Remove markdown and the word \"thought\" if it leaked\n",
    "    cleaned = cleaned.replace('*', '').replace('#', '').replace('`', '')\n",
    "    if cleaned.lower().startswith(\"thought\\n\"):\n",
    "        cleaned = cleaned[8:]\n",
    "        \n",
    "    # 5. Prevent the specific system action loop if it leaked\n",
    "    cleaned = cleaned.replace(\"[SYSTEM ACTION] Acknowledged.\", \"\")\n",
    "        \n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SUVI Orchestrator Core\n",
    "The main logic loop that decides whether to search the database, update records, or just chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T07:53:04.593787Z",
     "iopub.status.busy": "2026-02-23T07:53:04.593551Z",
     "iopub.status.idle": "2026-02-23T07:53:04.601100Z",
     "shell.execute_reply": "2026-02-23T07:53:04.600301Z",
     "shell.execute_reply.started": "2026-02-23T07:53:04.593763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  CELL 5: SUVI ORCHESTRATOR \n",
    "# ==============================================================================\n",
    "\n",
    "def run_suvi_agent(patient_id, user_text, image_context=\"\"):\n",
    "    tool_result = \"\"\n",
    "    \n",
    "    # 1. Intent Classification: Update Check\n",
    "    update_match = re.search(r\"(update|change|set)\\s+(weight|height|blood|allergy)\\s+(?:to\\s+)?(.+)\", user_text, re.IGNORECASE)\n",
    "    \n",
    "    if update_match:\n",
    "        field_map = {\"weight\": \"weight_kg\", \"height\": \"height_cm\", \"blood\": \"blood_group\", \"allergy\": \"allergies\"}\n",
    "        db_field = field_map.get(update_match.group(2).lower())\n",
    "        value = update_match.group(3).strip()\n",
    "        \n",
    "        action_status = update_patient_vitals(patient_id, db_field, value)\n",
    "        tool_result = f\"[SYSTEM: Vitals updated successfully.]\"\n",
    "        \n",
    "    else:\n",
    "        # 2. Intent Classification: RAG Search\n",
    "        history = search_patient_history(patient_id, user_text)\n",
    "        tool_result = f\"[MEDICAL CONTEXT]:\\n{history}\"\n",
    "\n",
    "    # 3. LLM Generation - STRICT PROMPT\n",
    "    # FIXED: Replaced {context_block} with {tool_result}\n",
    "    system_prompt = f\"\"\"You are Suvi, a Thoughtful, Intelligent and kind Ai Assistant for doctore.\n",
    "{tool_result}\n",
    "{image_context}\n",
    "\n",
    "CRITICAL INSTRUCTION: You are a reasoning model. You MUST wrap all of your internal thoughts, reasoning, and planning inside <think> and </think> tags. \n",
    "After the </think> tag, write your final, professional response to the doctor.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{system_prompt}\\n\\nDoctor says: {user_text}\"}\n",
    "    ]\n",
    "    \n",
    "    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=500, \n",
    "            do_sample=True, \n",
    "            temperature=0.1,          \n",
    "            repetition_penalty=1.15,  \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    # Extract ONLY the newly generated tokens\n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    raw_response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return clean_llm_output(raw_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAPI & Ngrok Server\n",
    "Exposes our agents to the Flutter frontend via REST API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T07:53:04.602441Z",
     "iopub.status.busy": "2026-02-23T07:53:04.602055Z",
     "iopub.status.idle": "2026-02-23T07:53:04.816828Z",
     "shell.execute_reply": "2026-02-23T07:53:04.816044Z",
     "shell.execute_reply.started": "2026-02-23T07:53:04.602407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CELL 6: FASTAPI SERVER & ENDPOINTS\n",
    "# ==============================================================================\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from fastapi import FastAPI, UploadFile, File, Form\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "\n",
    "app = FastAPI(title=\"AEGIS Medical API\")\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
    "\n",
    "# ENDPOINT 1: SUVI CHAT\n",
    "# ---------------------------------------------------------\n",
    "@app.post(\"/suvi/chat\")\n",
    "async def chat_endpoint(text: str = Form(...), patient_id: int = Form(...), file: UploadFile = File(None)):\n",
    "    try:\n",
    "        image_context = \"\"\n",
    "        if file:\n",
    "            image_context = \"[SYSTEM: User attached an image for review. Proceed carefully.]\"\n",
    "            \n",
    "        response = run_suvi_agent(patient_id, text, image_context)\n",
    "        return {\"response\": response}\n",
    "    except Exception as e:\n",
    "        return {\"response\": f\"System Error: {str(e)}\"}\n",
    "\n",
    "# ENDPOINT 2: AEGIS-VISION (FACE ID)\n",
    "# ---------------------------------------------------------\n",
    "@app.post(\"/suvi/identify_face\")\n",
    "async def identify_face(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        image_bytes = await file.read()\n",
    "        result = identify_patient_from_image(image_bytes)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# ENDPOINT 3: AEGIS-SCRIBE (REPORTING)\n",
    "# ---------------------------------------------------------\n",
    "@app.post(\"/suvi/generate_report\")\n",
    "async def generate_report(transcript: str = Form(...)):\n",
    "    try:\n",
    "        report = generate_clinical_report(transcript)\n",
    "        return {\"status\": \"success\", \"report\": report}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# START SERVER\n",
    "# ---------------------------------------------------------\n",
    "def start_server():\n",
    "    ngrok.kill()\n",
    "    if NGROK_TOKEN:\n",
    "        ngrok.set_auth_token(NGROK_TOKEN)\n",
    "    \n",
    "    tunnel = ngrok.connect(8000) \n",
    "    print(f\" API IS LIVE! UPDATE NGROK URL IN FLUTTER APP:\")\n",
    "    print(f\" {tunnel.public_url} \")\n",
    "    \n",
    "    config = uvicorn.Config(app, host=\"127.0.0.1\", port=8000, log_level=\"error\")\n",
    "    server = uvicorn.Server(config)\n",
    "    nest_asyncio.apply()\n",
    "    server.run()\n",
    "\n",
    "thread = threading.Thread(target=start_server)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  API Health Check & Debugging\n",
    "Run this cell to verify your database and local functions are working correctly before connecting the Flutter app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T07:53:04.818369Z",
     "iopub.status.busy": "2026-02-23T07:53:04.817774Z",
     "iopub.status.idle": "2026-02-23T07:53:57.413693Z",
     "shell.execute_reply": "2026-02-23T07:53:57.412989Z",
     "shell.execute_reply.started": "2026-02-23T07:53:04.818335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API IS LIVE! UPDATE NGROK URL IN FLUTTER APP:\n",
      " https://camelia-apocynaceous-pendantly.ngrok-free.dev \n",
      "---  RUNNING DIAGNOSTICS ---\n",
      " DB Connected. Found Patient: Rajesh Kumar\n",
      " SUVI Test Response: Okay, here is the response:\n",
      "\n",
      "Hello Doctor. Based on the information provided in the records, I understand you would like to know the patient's name. However, the records themselves do not include the patient's name. They refer to the patient generically as \"Patient\".\n",
      "\n",
      "Could you please confirm the patient's name when we next discuss them? If there's anything else I can assist you with regarding these records, feel free to ask.\n",
      "---  READY FOR FLUTTER CONNECTION ---\n"
     ]
    }
   ],
   "source": [
    "#  CELL 7: DIAGNOSTIC TESTS\n",
    "# ==============================================================================\n",
    "import time\n",
    "time.sleep(3) # Wait for server to boot\n",
    "\n",
    "print(\"---  RUNNING DIAGNOSTICS ---\")\n",
    "\n",
    "# 1. Test Supabase Connection\n",
    "res = supabase.table('patients').select('id, full_name').limit(1).execute()\n",
    "if res.data:\n",
    "    print(f\" DB Connected. Found Patient: {res.data[0]['full_name']}\")\n",
    "else:\n",
    "    print(\" DB Empty or Connection Failed.\")\n",
    "\n",
    "# 2. Test Orchestrator Logic\n",
    "test_response = run_suvi_agent(1, \"What is my patient's name?\")\n",
    "print(f\" SUVI Test Response: {test_response}\")\n",
    "\n",
    "print(\"---  READY FOR FLUTTER CONNECTION ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14898831,
     "sourceId": 118534,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
